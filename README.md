# LLEmon (Large Language Experimental Model)

A personal project where I'm experimenting with building a **basic Transformer-based language model from scratch**. The goal is to understand the internals of modern LLMs, including tokenization, training, sampling, and the Transformer architecture itself. 

---

## Features

- **Character-level tokenization**: Simple tokenizer that splits text into individual characters.
- **Transformer architecture**: Implemented from scratch using PyTorch, including multi-head attention, feedforward networks, and positional encoding.
- **Training loop**: Custom training loop with saving and loading model checkpoints.
- **Sampling**: Basic text generation from a trained model.
- **Data handling**: Utilities for loading and batching text data.
---